\section{Identification and Estimation}


\subsection{Identification}

\begin{frame}{Theorem 1}
\vspace{-0.5cm}
Let $X_i$ be a Gaussian process and $g : \mathbb{R}^S \to \mathbb{R}$ be an arbitrary
function with continuous partial derivatives almost everywhere such that 

$$
0 < \left| \expectation{\frac{\partial}{\partial x_s} g(X_i(\tau_1), \dots,
X_i(\tau_S))} \right| < \infty \,.
$$

Define
$$
\vartheta_s = \expectation{\frac{\partial}{\partial x_s} g(X_i(\tau_1), \dots,
X_i(\tau_S))}
$$
then,

$$
f_{XY}(t) = \expectation{X_i(t) Y_i} = \sum_{s=1}^S \vartheta_s \sigma(t, \tau_s)
$$
\end{frame}


\begin{frame}{Assumption 1}
    Given the kernel $\sigma$, there exists open $\Omega \supset [0, 1]^3$ and twice
    continuously differentiable function $\omega : \Omega \to \mathbb{R}$ as well as
    some $\kappa \in (0, 2)$ such that $\forall s, t \in [0, 1]$

    $$\sigma(s, t) = \omega(s, t, |s-t|^{\kappa}) \,.$$

    Moreover, $0 < \inf \left\{ c(t) : t \in [0, 1] \right\}$, where

    $$c(t) = -\frac{\partial}{\partial z} \omega(t, t, z)|_{z = 0} \,.$$
\end{frame}


\begin{frame}
    \vspace{-0.5cm}

    Can show that, under Assumption 1 and Theorem 1, as $\delta \to 0$

    \vspace{0.5cm}

    \[
    \expectation{Z_{\delta, i}(t) Y_i} = 
        \begin{cases}
            \mathcal{O}(\delta^\kappa) \quad, \text{if } t = \tau_s
            \text{ for some } \tau_s \in \{\tau_1, \dots, \tau_S\}\\
            \mathcal{O}(\delta^2) \quad, \text{else}
        \end{cases}
    \]

    \vspace{0.5cm}

    \[
    \frac{1}{n} \sum_{i = 1}^n Z_{\delta, i}(t) Y_i - \expectation{Z_{\delta, i}(t) Y_i}
    = \mathcal{O}_{\mathbb{P}} \left( \sqrt{\delta^{\kappa} / n} \right)
    \]

    \vspace{0.5cm}

    As seen in visualization, need sensible choice of $\delta$\\[1em]
    \labelitem $\delta$ too small (e.g. $\delta^\kappa \sim n^{-1}$) $\implies$
        estimation noise dominates\\
    \labelitem $\delta$ too big $\implies$ cannot distinguish between neighboring points


\end{frame}


\subsection{Estimation}

\begin{frame}{Estimation}
    \vspace{-1cm}
    \begin{table}[]
    \renewcommand{\arraystretch}{2}
        \begin{tabular}{ll}
          \labelitem $X_i$ observed over $p$ equidistant points $t_j$ in $[0, 1]$\\
          \labelitem Possibly $p \gg n$\\
          \labelitem Estimate $\{\tau_s\}$ using local maxima of $\,|\hat{f}_{ZY}(t)| = |n^{-1} \sum_{i=1}^n
          Z_{\delta, i}(t) Y_i |$\\
          \labelitem Algorithm 1: Given $\delta > 0$ determine $\hat{\tau}_1, \dots,
          \hat{\tau}_{M_{\delta}}$ with $M_{\delta} \in \mathbb{N}$
        \end{tabular}
    \end{table}

Finally,
\vspace{-0.8cm}
\begin{align*}
    \hat{S} &= \min \left\{\ell \in \mathbb{N}_0 : \left|\frac{\sum_{i = 1}^n Z_{\delta,
i}(\hat{\tau}_{\ell + 1}) Y_i}{\left\{\sum_{i=1}^n Z_{\delta, i} (\hat{\tau}_{\ell +
1})^2\right\}^{1/2}}
\right| < \lambda \right\} \,,
\end{align*}
and we select only the first $\hat{\tau}_s$, where $s = 1, \dots, \hat{S}$.

\end{frame}


\begin{frame}{Algorithm 1}

\vspace{-0.5cm}
\begin{algorithm}[H]
\begin{algorithmic}[1]
  \State compute $\hat{f}_{XY}(t_j) = \sum_i X_i(t_j) Y_i / n$, for all $j=1,\dots,p$
  \State choose $\delta > 0$ s.t. $\exists \, k_{\delta} \in \mathbb{N}$ with $1 \leq
  k_{\delta} < (p - 1)/2$ and $\delta = k_{\delta} / (p-1)$
  \State define $\mathcal{J}_{\delta} = \{k_{\delta} + 1, \dots, p - k_{\delta}\}$ and
  set $\ell = 1$
  \State compute $\hat{f}_{ZY}(t_j) = \hat{f}_{XY}(t_j) - (\hat{f}_{XY}(t_j + \delta) +
  \hat{f}_{XY}(t_j - \delta)) / 2$, for all $j \in \mathcal{J}_{\delta}$
  \While{$|\mathcal{J}_{\delta}| \neq \varnothing$}
  \State estimate $\hat{\tau}_{\ell} = \argmax \left\{|\,\hat{f}_{ZY}(t_j)| : \text{for }
      t_j \text{ with } j \in \mathcal{J}_{\delta}\right\}$
    \State update $\mathcal{J}_{\delta} \leftarrow \mathcal{J}_{\delta} \setminus
    [\hat{\tau}_{\ell} - \sqrt{\delta}, \hat{\tau}_{\ell} + \sqrt{\delta}]$
    \State update $\ell \leftarrow \ell + 1$
  \EndWhile
  \State \textbf{return} $\{\hat{\tau}_{\ell}\}$
\end{algorithmic}
\end{algorithm}

\end{frame}


\begin{frame}{Asymptotics}
    Can we say something about the convergence rate of $\hat{\tau}_s$?\\[1em]\pause
    Assumption 1 + 2 and Theorem 1 + 2 $\implies$ superconsistent rates
\end{frame}


\begin{frame}{Assumption 2}

    \vspace{-1cm}
    \begin{table}[]
    \renewcommand{\arraystretch}{2}
        \begin{tabular}{l}
            \blue{(a)} $X_1, \dots, X_n \stackrel{iid}{\sim} X$, where $X$ is a
            Gaussian process\\
            \blue{(b)} $\exists \, 0 < \sigma_{|y|} < \infty$ s.t. $\forall m \geq 1:\,
            \expectation{|Y_i|^{2m}} \leq 2^{m-1} m! (\sigma_{|y|})^{2m}$
        \end{tabular}
    \end{table}

Condition \blue{(b)} is fulfilled, for example,if $Y_i$ is bounded, as in the logistic
regression case, or if \blue{(a)} holds and the errors $\epsilon_i$ are sub-Gaussian and
$g$ has bounded partial derivatives.

\end{frame}


\begin{frame}{Theorem 2}{(Under Assumption 1, 2 and Theorem 1)}

    If $\delta = \delta_n \to 0$ as $n \to \infty$ with $n \delta^{\kappa} /
    |\log\delta| \to \infty$ and $\delta^{\kappa} / n^{-\kappa + 1} \to 0$, then

    \begin{table}
    \renewcommand{\arraystretch}{2}
        \begin{tabular}{l}
            \blue{(i)} $\max_{\ell=1,\dots,\hat{S}} \min_{s=1,\dots, S}
            |\hat{\tau}_{\ell} - \tau_s| =
            \mathcal{O}_{\mathbb{P}}(n^{-1/\kappa})$\\
            \blue{(ii)} $\exists \, D \in (0, \infty)$ such that when algorithm 1 is
            applied with threshold\\
            \quad $\lambda = \lambda_n = A \sqrt{\sigma_{|y|}^2 / n
            \log \left (1 / \delta \right)}$, with $A > D$, and $\delta^2 =
            \mathcal{O}(n^{-1})$, then,\\
            \quad $\mathbb{P}\left[\hat{S} = S\right] \to 1$ as $n \to \infty$.
        \end{tabular}
    \end{table}


\end{frame}
