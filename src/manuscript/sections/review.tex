\section{Review}
\label{section:review}

In this section I present the general setting of \cite{Kneip2020}, albeit restricted to
the definitions, assumptions and theorems that are needed to understand the extension in
section \ref{section:extension}. We assume there is an independent and identically
distributed sample of data $(X_i, y_i)$ for $i=1,\dots,n$ individuals. The functional
regressors $X_i = \{ X_i(t) : t \in [a, b] \}$ is assumed to be a square-integrable
process and $y_i$ is a real-valued random variable. The relationship between regressors
and outcome is modeled as

\[
    y_i = g \left( X_i(\tau_1), \dots, X_i(\tau_S) \right) + \epsilon_i \,,
\]

with $\epsilon_i$ representing the error term satisfying $\expectation{\epsilon_i \mid
X_i(t)} = 0$ for all $t \in [a, b]$. The points of impact are denoted by $\tau_1, \dots,
\tau_S$; where the specific locations $\tau_s \in [a, b]$ are assumed to be unknown a
priori, as well as the number of points $S \in \mathbb{N}_0$. In the same way the link
function $g$ is assumed to be unknown as well. The paper considers centred random
functions $X_i$.

Now one may question how it is possible to estimate the locations as well as their
number while allowing the link function $g$ to be flexible. The upcoming repetitions of
the main assumptions and theorems in \cite{Kneip2020} illustrate that restrictions on
$X_i$ through the covariance kernel suffice for the identification and estimation.

Consider first the covariance kernel of the functional regressor. Let $\sigma(t, s) =
\expectation{X_i(t) X_i(s)}$ denote the covariance kernel of $X_i$.


\begin{assumption}
    Given the kernel $\sigma$, there exists open $\Omega \subset [0, 1]^3$ and twice
    continuously differentiable function $\omega : \Omega \to \mathbb{R}$, as well as
    some $\kappa \in (0, 2)$, such that $\forall s, t \in [0, 1]$

    \[
        \sigma(s, t) = \omega(s, t, |s-t|^{\kappa}) \,.
    \]

    Moreover, $0 < \inf \left\{ c(t) : t \in [0, 1] \right\}$, where $c(t) =
    -\frac{\partial}{\partial z} \omega(t, t, z)|_{z = 0} \,.$
\label{assumption:1}
\end{assumption}

Assumption \ref{assumption:1} restricts the degree of smoothness at the diagonal using
the parameter $\kappa$. This in turn implies certain behavior of the sample paths of the
process. Values with $\kappa < 2$ suggest non-smooth trajectories, while processes with
smooth sample paths and twice continuously differentiable kernel will satisfy the
assumption with $\kappa = 2$. That is, in the current form assumption \ref{assumption:1}
implies that the sample paths of the regressors $X_i$ need to be somewhat rough. Many
known processes fulfill assumption \ref{assumption:1}, for example, Brownian Motion
fulfills it with $\kappa = 1$. In section \ref{section:extension} I present an extension
to this assumption with $\kappa > 2$, allowing for a greater number of processes to be
modeled.


The ability to identify and estimate the points-of-impact relies heavily on the
decomposition presented in theorem \ref{theorem:1}.


\begin{theorem}
Let $X_i$ be a Gaussian process and $g : \mathbb{R}^S \to \mathbb{R}$ an arbitrary
function with continuous partial derivatives almost everywhere. For $s=1,\dots,S$ define

\[
    \vartheta_s = \expectation{\frac{\partial}{\partial x_s} g(X_i(\tau_1), \dots,
    X_i(\tau_S))} \,.
\]

If $0 < \left| \vartheta_s \right| < \infty, \forall s=1,\dots,S$, then we may write,
$\forall t \in [a, b]$

\[
    f_{XY}(t) \stackrel{def}{=} \expectation{X_i(t) y_i} = \sum_{s=1}^S \vartheta_s
    \sigma(t, \tau_s) \,.
\]
\label{theorem:1}
\end{theorem}


Let us henceforth call $f_{XY}$ the cross-covariance (between $X_i$ and $y_i$). In light
of assumption \ref{assumption:1} we know that the kernel $\sigma$ is \emph{not}
two-times differentiable at the diagonal. But then $f_{XY}$ will not be two-times
differentiable at the points-of-impact $\tau_1, \dots, \tau_S$. As it turns out, this
will be enough to ensure identification of the points-of-impact. To differentiate
between any point ($t \in [a, b]$) and a point-of-impact ($t = \tau_s$ for some $s$) the
paper proposes the measure

\[
    f_{ZY}(t) \stackrel{def}{=} f_{XY}(t) - \frac{1}{2} \left( f_{XY}(t + \delta) +
    f_{XY}(t - \delta) \right) \,,
\]

with hyper-parameter $\delta > 0$. Under our current set of assumptions the function
$f_{ZY}$ will be large in absolute value for $t$ close to a point-of-impact. This allows
for the estimation of the points-of-impact using extremum points of an estimated version
of $|f_{ZY}|$. But what is $f_{ZY}$ actually measuring? Notice that the second-order
finite difference of $f_{XY}$ is given by

\begin{align*}
    FD(f_{XY}, \delta, 2)(x) &= \left( f(x + \delta) + f(x - \delta) \right) - 2 f(x)\\
                             &= - \frac{1}{2} f_{ZY}(x) \,,
\end{align*}

where $FD(h, \delta, \ell, x)$ denotes the $\ell$-th order finite difference of function
$h$ with step length $\delta$, evaluated at $x$. Since the criterion function is in
absolute value we can see that using $f_{ZY}$ is proportional to second-order finite
difference of $f_{XY}$. This should make sense, as assumption \ref{assumption:1} implied
that $f_{XY}$ should not be two-times differentiable at the points-of-impact. In that
case, the second-order finite difference formula is expected to be larger at the
points-of-impact than at the other time points.


With this new interpreation in mind we may ask whether we can exploite higher-order
finite differences to relax the assumption on the covariance kernel? The answer is
affirmative and the details will be the concern of section \ref{section:extension}.


At last I state the original version of the estimation algorithm as a comparison to the
more general version supplied in the next section. For the estimation stage we suppose
that the functional regressors $X_i$ are observed at $p$ equidistant points $t_1, \dots,
t_p$ with $t_1 = a, t_p = b$. Since the functions $f_{XY}$ and $f_{ZY}$ are simple
functions of expectations, they can be easily estimated by the standard sample
counterpart, which we denote by $\hat{f}_{XY}$ and $\hat{f}_{ZY}$. The orignal algorithm
is listed as algorithm \ref{algorithm:1}.

\begin{tcolorbox}[standard jigsaw, opacityback=0]

\begin{algorithm}[H]
\caption{Original algorithm from \cite{Kneip2020}, adapted for readability.}
\label{algorithm:1}
\begin{algorithmic}[1]
  \State compute $\hat{f}_{XY}(t_j) = \sum_i X_i(t_j) Y_i / n$, for all $j=1,\dots,p$
  \State choose $\delta > 0$ s.t. $\exists \, k_{\delta} \in \mathbb{N}$ with $1 \leq
  k_{\delta} < (p - 1)/2$ and $\delta = k_{\delta} / (p-1)$
  \State define $\mathcal{J}_{\delta} = \{k_{\delta} + 1, \dots, p - k_{\delta}\}$ and
  set $\ell = 1$
  \State compute $\hat{f}_{ZY}(t_j) = \hat{f}_{XY}(t_j) - (\hat{f}_{XY}(t_j + \delta) +
  \hat{f}_{XY}(t_j - \delta)) / 2$, for all $j \in \mathcal{J}_{\delta}$
  \While{$\mathcal{J}_{\delta} \neq \varnothing$}
  \State estimate $\hat{\tau}_{\ell} = \argmax \left\{|\,\hat{f}_{ZY}(t_j)| : \text{for }
      t_j \text{ with } j \in \mathcal{J}_{\delta}\right\}$
    \State update $\mathcal{J}_{\delta} \leftarrow \mathcal{J}_{\delta} \setminus
    [\hat{\tau}_{\ell} - \sqrt{\delta}, \hat{\tau}_{\ell} + \sqrt{\delta}]$
    \State update $\ell \leftarrow \ell + 1$
  \EndWhile
  \State \textbf{return} $\{\hat{\tau}_{\ell}\}$
\end{algorithmic}
\end{algorithm}

\end{tcolorbox}
