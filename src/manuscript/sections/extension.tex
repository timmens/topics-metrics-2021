\section{Extension}
\label{section:extension}

In this section I consider a generalization which was touched upon in the previous
section. This corresponds to the generalization discussed in subsection 2.4.2
(\emph{Generalizing covariance assumption 1}) of \cite{Kneip2020}.

One of the main restrictions of Assumption \ref{assumption:1} is that it excludes
smooth, twice continuously differentiable processes with $\kappa \geq 2$. On first
thought, it may be intuitive that a certain degree of \emph{local variation} is
necessary for identification; as also discussed in \cite{Kneip2016}. However, in light
of Theorem \ref{theorem:1} this is not necessary. As discussed in the previous section,
a sensible criterion function inherits the properties of the covariance kernel at the
diagonal to allow for the differentiation between regular time points and
points-of-impact. Before, under Assumption \ref{assumption:1} we presumed that the
kernel is twice continuously differentiable off the diagonal and not twice continuously
differentiable on the diagonal. But this can be relaxed to the rather general case where
the kernel is less smooth on the diagonal than off the diagonal. For example, we may
extend the assumption to the case $\kappa < 4$, which then implies that the kernel may
be four-times continuously differentiable off the diagonal but not on the diagonal.
Given this modified assumption we would then have to adapt the definition of $f_{ZY}$
and hence the criterion function.

This generalization may be used for any even $d = 2, 4, 6, 8, \dots$ while using
Assumption \ref{assumption:1} with $\kappa < d$. For each $d$ the natural extension of
the criterion function then involves the $d$-th order finite difference. Again, let
$FD(h, \delta, k, x)$ denote the $k$-th order finite difference of function $h$ with
step size $\delta$ at $x$. Algorithm \ref{algorithm:1} can then be generalized by
updating lines 3 and 4. Since higher-order finite difference computations need multiple
steps in the argument, the grid spanned in line 3 has to be adjusted. Most importantly
the second-order finite difference formula from line 4 is exchanged for the general
case. The updated version is listed as Algorithm \ref{algorithm:2}. This version of the
algorithm is used in the Monte-Carlo study (Section \ref{section:monte_carlo}) and is
available online: \url{https://github.com/timmens/fdapoi}.

\begin{tcolorbox}[standard jigsaw, opacityback=0]

\begin{algorithm}[H]
\caption{Generalization of Algorithm \ref{algorithm:1}.}
\label{algorithm:2}

\begin{algorithmic}[1]
  \State compute $\hat{f}_{XY}(t_j) = \sum_i X_i(t_j) Y_i / n$, for all $j=1,\dots,p$
  \State choose $\delta > 0$ s.t. $\exists \, k_{\delta} \in \mathbb{N}$ with $1 \leq
  k_{\delta} < (p - 1)/2$ and $\delta = k_{\delta} / (p-1)$
  \State define $\mathcal{J}_{\delta} = \Call{ComputeGrid}{p, k_{\delta}, d}$ and set
  $\ell = 1$
  \State compute $\hat{f}^\ast(t_j) = FD(\hat{f}_{XY}, \delta, d, t_j) \,$ for all
  $j \in \mathcal{J}_{\delta}$
  \While{$\mathcal{J}_{\delta} \neq \varnothing$}
  \State estimate $\hat{\tau}_{\ell} = \argmax \left\{|\,\hat{f}^\ast(t_j)| : \text{for
    } t_j \text{ with } j \in \mathcal{J}_{\delta}\right\}$
    \State update $\mathcal{J}_{\delta} \leftarrow \mathcal{J}_{\delta} \setminus
    [\hat{\tau}_{\ell} - \sqrt{\delta}, \hat{\tau}_{\ell} + \sqrt{\delta}]$
    \State update $\ell \leftarrow \ell + 1$
  \EndWhile
  \State \textbf{return} $\{\hat{\tau}_{\ell}\}$
\end{algorithmic}
\end{algorithm}

\end{tcolorbox}

Using arbitrary large values for $d$ does not come without costs, though. There are two
main potential problems. One, higher-order finite differences can be numerically
unstable, especially in our case where $\delta$ is usually chosen much larger than in
the standard case of derivative approximation. And two, for higher-order differences
fewer points in $\{t_1, \dots, t_p\}$ can be used, because an evaluation of the finite
difference formula requires more and more points to the left and right of the evaluation
point. This latter point implies that the algorithm becomes blind to points-of-impact
close to the boundaries $a$ and $b$.

\smallskip

My implementation of general higher-order central differences is taken from
\cite{Jordan1965}, which further provides a helpful introduction to the topic. The
formula is given by

\[
    FD(f, \delta, k, x) = \sum_{j = 0}^k (-1)^j \binom{k}{j} f \left( x + \left[
    \frac{k}{2} - j \right] \delta \right) \,.
\]
